# -*- coding: utf-8 -*-
"""LightFM_ModelTraining

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sqnl4IZbyQn8S-K0r4ltkBzuxfUkAfw8
"""



"""## Install LightFM Model"""

pip install git+https://github.com/daviddavo/lightfm

"""## Import Libariries"""

from scipy import sparse
from lightfm import LightFM
from lightfm.data import Dataset
from lightfm.evaluation import precision_at_k, recall_at_k, auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import itertools
import warnings
warnings.filterwarnings('ignore')

"""##  TEXT FEATURE EXTRACTION WITH TF-IDF"""

def extract_text_features(df, max_features=50):
    """Extract TF-IDF features from product descriptions"""
    print("\n" + "=" * 80)
    print("STEP 3: TEXT FEATURE EXTRACTION (TF-IDF)")
    print("=" * 80)

    # Get unique products with descriptions
    unique_products = df.groupby('ProductKey')['ProductDescription'].first().reset_index()

    # TF-IDF vectorization
    tfidf = TfidfVectorizer(max_features=max_features, stop_words='english',
                            ngram_range=(1, 2), min_df=2)

    tfidf_matrix = tfidf.fit_transform(unique_products['ProductDescription'].fillna(''))

    # Create text feature dataframe
    text_features_df = pd.DataFrame(
        tfidf_matrix.toarray(),
        columns=[f'text_{word}' for word in tfidf.get_feature_names_out()]
    )
    text_features_df['ProductKey'] = unique_products['ProductKey'].values

    # Merge back to main dataframe
    df = df.merge(text_features_df, on='ProductKey', how='left')

    print(f"Extracted {max_features} TF-IDF features from product descriptions")
    print(f"Top terms: {list(tfidf.get_feature_names_out()[:10])}")

    return df, list(text_features_df.columns[:-1])

"""## PREPARE DATA FOR LIGHTFM (IMPROVED)"""

def prepare_lightfm_data(df, text_feature_cols):
    """Prepare comprehensive interaction and feature matrices for LightFM"""
    print("\n" + "=" * 80)
    print("STEP 4: PREPARING ENHANCED DATA FOR LIGHTFM")
    print("=" * 80)

    dataset = Dataset()

    # Fit the dataset with users and items
    dataset.fit(
        users=df['CustomerKey'].unique(),
        items=df['ProductKey'].unique()
    )

    # ===== COMPREHENSIVE USER FEATURES =====
    user_features_list = [
        'Gender', 'MaritalStatus', 'EducationLevel', 'Occupation',
        'HomeOwner', 'IncomeBracket', 'ChildrenCategory', 'CustomerSegment', 'Season'
    ]

    # ===== COMPREHENSIVE ITEM FEATURES =====
    item_features_list = ['ModelName', 'ItemCategory', 'PopularityPercentile']

    # Build user features
    user_feature_tuples = []
    for customer_key, group in df.groupby('CustomerKey'):
        row = group.iloc[0]
        features = [f"{feat}:{row[feat]}" for feat in user_features_list if feat in row]

        # Add binned numerical features
        if 'TotalOrders' in row:
            order_bin = 'HighActivity' if row['TotalOrders'] > 5 else 'MedActivity' if row['TotalOrders'] > 2 else 'LowActivity'
            features.append(f"Activity:{order_bin}")

        user_feature_tuples.append((customer_key, features))

    # Build item features with text features
    item_feature_tuples = []
    for product_key, group in df.groupby('ProductKey'):
        row = group.iloc[0]
        features = [f"{feat}:{row[feat]}" for feat in item_features_list if feat in row]

        # Add top TF-IDF features (only significant ones)
        for text_col in text_feature_cols[:20]:  # Top 20 text features
            if text_col in row and row[text_col] > 0.1:  # Threshold for relevance
                features.append(f"{text_col}")

        item_feature_tuples.append((product_key, features))

    # Fit features
    all_user_features = [f for _, feats in user_feature_tuples for f in feats]
    all_item_features = [f for _, feats in item_feature_tuples for f in feats]

    dataset.fit_partial(
        users=df['CustomerKey'].unique(),
        items=df['ProductKey'].unique(),
        user_features=all_user_features,
        item_features=all_item_features
    )

    # Build feature matrices
    user_features_matrix = dataset.build_user_features(user_feature_tuples)
    item_features_matrix = dataset.build_item_features(item_feature_tuples)

    print(f"User features matrix shape: {user_features_matrix.shape}")
    print(f"Item features matrix shape: {item_features_matrix.shape}")
    print(f"Total user features: {len(set(all_user_features))}")
    print(f"Total item features: {len(set(all_item_features))}")

    return dataset, user_features_matrix, item_features_matrix


def create_interaction_matrices(df, dataset):
    """Create train and test interaction matrices with temporal split"""
    print("\n" + "=" * 80)
    print("STEP 5: CREATING TRAIN/TEST SPLITS")
    print("=" * 80)

    # Sort by date for temporal split
    df_sorted = df.sort_values('OrderDate')
    split_idx = int(len(df_sorted) * 0.8)

    train_df = df_sorted.iloc[:split_idx]
    test_df = df_sorted.iloc[split_idx:]

    # Build interaction matrices with weights
    train_interactions, train_weights = dataset.build_interactions(
        [(row['CustomerKey'], row['ProductKey'], row['OrderQuantity'])
         for _, row in train_df.iterrows()]
    )

    test_interactions, test_weights = dataset.build_interactions(
        [(row['CustomerKey'], row['ProductKey'], row['OrderQuantity'])
         for _, row in test_df.iterrows()]
    )

    print(f"Train interactions: {train_interactions.shape}, density: {train_interactions.nnz / np.prod(train_interactions.shape):.6f}")
    print(f"Test interactions: {test_interactions.shape}, density: {test_interactions.nnz / np.prod(test_interactions.shape):.6f}")
    print(f"Train samples: {len(train_df)}, Test samples: {len(test_df)}")

    return train_interactions, test_interactions, train_weights, test_weights

"""## ADVANCED MODEL TRAINING WITH REGULARIZATION"""

def train_lightfm_model(train_interactions, user_features, item_features,
                        train_weights=None, loss='warp', no_components=50,
                        learning_rate=0.05, item_alpha=0.0001, user_alpha=0.0001,
                        epochs=50, num_threads=4, verbose=True):
    """Train LightFM model with regularization to prevent overfitting"""

    model = LightFM(
        no_components=no_components,
        learning_rate=learning_rate,
        loss=loss,
        item_alpha=item_alpha,  # L2 penalty for item features
        user_alpha=user_alpha,  # L2 penalty for user features
        random_state=42
    )

    model.fit(
        train_interactions,
        user_features=user_features,
        item_features=item_features,
        sample_weight=train_weights,
        epochs=epochs,
        num_threads=num_threads,
        verbose=verbose
    )

    return model


def extended_hyperparameter_search(train_interactions, test_interactions,
                                   user_features, item_features, train_weights):
    """Extended grid search with regularization parameters"""
    print("\n" + "=" * 80)
    print("STEP 6: ADVANCED HYPERPARAMETER TUNING WITH REGULARIZATION")
    print("=" * 80)

    param_grid = {
        'no_components': [30, 50, 70],
        'learning_rate': [0.03, 0.05, 0.08],
        'loss': ['warp'],  # WARP is best for implicit feedback
        'item_alpha': [0.0, 0.00001, 0.0001],
        'user_alpha': [0.0, 0.00001, 0.0001]
    }

    best_score = 0
    best_params = {}
    results = []

    # Sample combinations for efficiency
    print("Testing regularization combinations to reduce overfitting...\n")

    combinations = [
        (50, 0.05, 'warp', 0.0, 0.0),        # Baseline
        (50, 0.05, 'warp', 0.00001, 0.00001), # Light regularization
        (50, 0.05, 'warp', 0.0001, 0.0001),   # Medium regularization
        (70, 0.03, 'warp', 0.0001, 0.0001),   # More components + regularization
        (30, 0.08, 'warp', 0.0001, 0.0001),   # Fewer components + regularization
        (50, 0.05, 'warp', 0.001, 0.001),     # Strong regularization
    ]

    for i, (n_comp, lr, loss, i_alpha, u_alpha) in enumerate(combinations, 1):
        print(f"[{i}/{len(combinations)}] Testing: comp={n_comp}, lr={lr}, loss={loss}, "
              f"item_alpha={i_alpha}, user_alpha={u_alpha}")

        model = train_lightfm_model(
            train_interactions, user_features, item_features,
            train_weights=train_weights, loss=loss, no_components=n_comp,
            learning_rate=lr, item_alpha=i_alpha, user_alpha=u_alpha,
            epochs=30, num_threads=4, verbose=False
        )

        # Evaluate on both train and test
        train_precision = precision_at_k(model, train_interactions,
                                        user_features=user_features,
                                        item_features=item_features, k=10).mean()
        test_precision = precision_at_k(model, test_interactions,
                                       user_features=user_features,
                                       item_features=item_features, k=10).mean()
        train_auc = auc_score(model, train_interactions,
                             user_features=user_features,
                             item_features=item_features).mean()
        test_auc = auc_score(model, test_interactions,
                            user_features=user_features,
                            item_features=item_features).mean()

        overfitting_gap = train_auc - test_auc

        results.append({
            'no_components': n_comp,
            'learning_rate': lr,
            'loss': loss,
            'item_alpha': i_alpha,
            'user_alpha': u_alpha,
            'train_precision@10': train_precision,
            'test_precision@10': test_precision,
            'train_auc': train_auc,
            'test_auc': test_auc,
            'overfitting_gap': overfitting_gap
        })

        print(f"  Train Precision@10: {train_precision:.4f}, Test: {test_precision:.4f}")
        print(f"  Train AUC: {train_auc:.4f}, Test AUC: {test_auc:.4f}")
        print(f"  Overfitting Gap: {overfitting_gap:.4f}\n")

        # Best based on test precision and low overfitting
        score = test_precision - (0.1 * overfitting_gap)  # Penalize overfitting
        if score > best_score:
            best_score = score
            best_params = {
                'no_components': n_comp,
                'learning_rate': lr,
                'loss': loss,
                'item_alpha': i_alpha,
                'user_alpha': u_alpha
            }

    results_df = pd.DataFrame(results)
    print(f"Best parameters (balancing performance and overfitting): {best_params}")
    print(f"Best adjusted score: {best_score:.4f}\n")

    return best_params, results_df