# -*- coding: utf-8 -*-
"""VISUALIZATION_COMPARISON

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sqnl4IZbyQn8S-K0r4ltkBzuxfUkAfw8
"""



"""## VISUALIZATION & COMPARISON"""

def plot_comparison_results(baseline_results, improved_results):
    """Visualize improvements from baseline to improved model"""
    print("\n" + "=" * 80)
    print("STEP 9: PERFORMANCE COMPARISON & VISUALIZATION")
    print("=" * 80)

    fig, axes = plt.subplots(2, 2, figsize=(16, 10))

    # Precision comparison
    ax1 = axes[0, 0]
    metrics = ['precision@5', 'precision@10', 'precision@20']
    baseline_vals = [baseline_results[m] for m in metrics]
    improved_vals = [improved_results[m] for m in metrics]

    x = np.arange(len(metrics))
    width = 0.35
    ax1.bar(x - width/2, baseline_vals, width, label='Baseline', color='steelblue', alpha=0.7)
    ax1.bar(x + width/2, improved_vals, width, label='Improved', color='seagreen', alpha=0.7)
    ax1.set_xlabel('Metric')
    ax1.set_ylabel('Score')
    ax1.set_title('Precision Comparison: Baseline vs Improved')
    ax1.set_xticks(x)
    ax1.set_xticklabels(['P@5', 'P@10', 'P@20'])
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)

    # Recall comparison
    ax2 = axes[0, 1]
    metrics = ['recall@5', 'recall@10', 'recall@20']
    baseline_vals = [baseline_results[m] for m in metrics]
    improved_vals = [improved_results[m] for m in metrics]

    ax2.bar(x - width/2, baseline_vals, width, label='Baseline', color='steelblue', alpha=0.7)
    ax2.bar(x + width/2, improved_vals, width, label='Improved', color='seagreen', alpha=0.7)
    ax2.set_xlabel('Metric')
    ax2.set_ylabel('Score')
    ax2.set_title('Recall Comparison: Baseline vs Improved')
    ax2.set_xticks(x)
    ax2.set_xticklabels(['R@5', 'R@10', 'R@20'])
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)

    # AUC comparison
    ax3 = axes[1, 0]
    metrics = ['train_auc', 'test_auc']
    baseline_vals = [baseline_results[m] for m in metrics]
    improved_vals = [improved_results[m] for m in metrics]

    x_auc = np.arange(len(metrics))
    ax3.bar(x_auc - width/2, baseline_vals, width, label='Baseline', color='steelblue', alpha=0.7)
    ax3.bar(x_auc + width/2, improved_vals, width, label='Improved', color='seagreen', alpha=0.7)
    ax3.set_xlabel('Dataset')
    ax3.set_ylabel('AUC Score')
    ax3.set_title('AUC Comparison: Baseline vs Improved')
    ax3.set_xticks(x_auc)
    ax3.set_xticklabels(['Train AUC', 'Test AUC'])
    ax3.legend()
    ax3.grid(axis='y', alpha=0.3)

    # Overfitting gap comparison
    ax4 = axes[1, 1]
    metrics = ['overfitting_gap']
    baseline_vals = [baseline_results[m] for m in metrics]
    improved_vals = [improved_results[m] for m in metrics]

    x_gap = np.arange(len(metrics))
    ax4.bar(x_gap - width/2, baseline_vals, width, label='Baseline', color='red', alpha=0.7)
    ax4.bar(x_gap + width/2, improved_vals, width, label='Improved', color='orange', alpha=0.7)
    ax4.set_xlabel('Model')
    ax4.set_ylabel('Overfitting Gap')
    ax4.set_title('Overfitting Reduction: Baseline vs Improved')
    ax4.set_xticks(x_gap)
    ax4.set_xticklabels(['Overfitting Gap'])
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.show()


def plot_hyperparameter_analysis(results_df):
    """Analyze the effect of different hyperparameters"""
    print("\n HYPERPARAMETER ANALYSIS")
    print("=" * 80)

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Effect of regularization on overfitting
    ax1 = axes[0, 0]
    results_df['total_alpha'] = results_df['item_alpha'] + results_df['user_alpha']
    ax1.scatter(results_df['total_alpha'], results_df['overfitting_gap'],
                c=results_df['test_precision@10'], cmap='viridis', s=100)
    ax1.set_xlabel('Total Regularization (item_alpha + user_alpha)')
    ax1.set_ylabel('Overfitting Gap')
    ax1.set_title('Effect of Regularization on Overfitting')
    ax1.grid(True, alpha=0.3)

    # Effect of components on performance
    ax2 = axes[0, 1]
    for n_comp in results_df['no_components'].unique():
        subset = results_df[results_df['no_components'] == n_comp]
        ax2.scatter(subset['test_precision@10'], subset['test_auc'],
                   label=f'{n_comp} components', s=80)
    ax2.set_xlabel('Test Precision@10')
    ax2.set_ylabel('Test AUC')
    ax2.set_title('Components vs Performance')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Learning rate analysis
    ax3 = axes[1, 0]
    results_df.groupby('learning_rate')['test_precision@10'].mean().plot(
        kind='bar', ax=ax3, color='purple', alpha=0.7)
    ax3.set_xlabel('Learning Rate')
    ax3.set_ylabel('Average Test Precision@10')
    ax3.set_title('Learning Rate Impact on Precision')
    ax3.grid(axis='y', alpha=0.3)

    # Best configuration summary
    ax4 = axes[1, 1]
    best_config = results_df.loc[results_df['test_precision@10'].idxmax()]
    metrics = ['test_precision@10', 'test_auc', 'overfitting_gap']
    values = [best_config[m] for m in metrics]
    colors = ['green', 'blue', 'orange']
    bars = ax4.bar(metrics, values, color=colors, alpha=0.7)
    ax4.set_ylabel('Score')
    ax4.set_title('Best Configuration Performance')
    ax4.grid(axis='y', alpha=0.3)

    # Add value labels on bars
    for bar, value in zip(bars, values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.4f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()